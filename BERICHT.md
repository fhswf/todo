# Dokumentation Softwarequalit√§t
| Name                 | Matrikelnummer        |
|----------------------|-----------|
| Christopher Schr√∂pfer | 30267385 |
| Harun Silajdzija      | 30265109 |
| Philipp Spiertz       | 30253272 |

## Dokumentation der Erstellung von Unit-Tests f√ºr das Backend | Name: Christopher Schr√∂pfer | Matrikelnummer: 30267385

### Vorgehensweise

Im Rahmen der Aufgabenstellung wurde die Backend-API um Unit-Tests erg√§nzt und bestehende Funktionalit√§ten verbessert. Ziel war es, eine gute Testabdeckung der API zu gew√§hrleisten, indem Eingabedaten validiert und typische Anwendungs- und Fehlerf√§lle abgedeckt werden.

Zun√§chst wurde die bestehende Codebasis analysiert, um eine √úbersicht √ºber die vorhandenen Funktionen und m√∂glichen Fehlerquellen zu erhalten. Dabei wurden die folgenden relevanten Endpunkte identifiziert:

- `GET /todos`
- `GET /todos/:id`
- `PUT /todos/:id`
- `POST /todos`
- `DELETE /todos/:id`

F√ºr diese Endpunkte wurden Tests entworfen, die sowohl positive Szenarien (korrekte Eingabedaten) als auch negative Szenarien (fehlerhafte oder unvollst√§ndige Eingaben) abdecken.

Vor Beginn der Arbeiten waren bereits grundlegende Tests vorhanden. Diese Tests deckten die Authentifizierung sowie die grundlegende Funktionalit√§t der Endpunkte ab. Sie pr√ºften positive Szenarien, wie z. B. das Abrufen oder Erstellen von Todos, sowie negative Szenarien, wie beispielsweise die Abfrage eines Todos anhand einer ung√ºltigen ID.

Die Implementierung der Tests erfolgte mit dem Framework supertest, das eine einfache M√∂glichkeit bietet, HTTP-Anfragen an die API zu simulieren. Dabei wurde supertest in Kombination mit dem Test-Framework jest eingesetzt. W√§hrend supertest die eigentlichen HTTP-Anfragen erstellt und die API-Endpunkte testet, √ºbernimmt jest die Rolle des Test-Runners und stellt die Assertions sowie die Ausf√ºhrung und Organisation der Tests sicher.

Diese Kombination erm√∂glicht ein effizientes Testen der API, da supertest die API-Aufrufe wie ein echter Client ausf√ºhrt, w√§hrend jest sicherstellt, dass die Ergebnisse den Erwartungen entsprechen. Beispielsweise wird √ºberpr√ºft, ob die API den korrekten HTTP-Statuscode zur√ºckgibt oder ob die Antwort die erwarteten Felder enth√§lt.

Zus√§tzlich wurde die Backend-Validierung erweitert, um Fehler fr√ºhzeitig abzufangen und pr√§zisere Fehlermeldungen an den Client zu liefern.

---

### Gew√§hlte L√∂sungen

#### 1. Erweiterung der Validierung
Die Validierung der Eingabedaten wurde um folgende Aspekte erg√§nzt:

- Pr√ºfung auf korrektes Format der MongoDB-IDs (`isMongoId`).
- Sicherstellung, dass alle erforderlichen Felder (`title`, `due`, `status`) vorhanden und vom korrekten Typ sind.
- Beschr√§nkung der erlaubten Zeichenl√§nge f√ºr das Feld `title` auf 3‚Äì100 Zeichen.
- Validierung des Statusfelds, um nur Werte zwischen 0 und 2 zu akzeptieren.
- Pr√ºfung von Datumsfeldern auf Konformit√§t zur ISO 8601.

Diese Validierungsregeln wurden mit Hilfe von `express-validator` definiert und bei den entsprechenden Endpunkten eingebunden. Dadurch wird sichergestellt, dass nur valide Daten in die Datenbank gelangen.

#### 2. Implementierung der Unit-Tests
F√ºr jeden Endpunkt wurden mehrere Testszenarien erstellt:

- **GET /todos**:
  - Erfolgreicher Abruf aller Todos.
  - Fehler bei fehlerhaften Format der ID.

- **POST /todos**:
  - Erfolgreiche Erstellung eines neuen Todos.
  - Fehler bei unvollst√§ndigen Eingaben.
  - Fehler bei zus√§tzlichen nicht erlaubten Feldern.
  - Fehler bei der Angabe von Feldern mit einem falschen Datentyp.

- **GET /todos/:id**:
  - Erfolgreicher Abruf eines Todos.
  - Fehler bei Angabe einer nicht existierenden ID.

- **PUT /todos/:id**:
  - Erfolgreiche Aktualisierung eines Todos.
  - Fehler bei falschen Datentypen.
  - Fehler bei Angabe einer nicht existierenden ID.

- **DELETE /todos/:id**:
  - Erfolgreiche L√∂schung eines Todos.
  - Fehler bei Angabe einer nicht existierenden ID.

#### 3. Verbesserung der API-Fehlermeldungen
Die API wurde so angepasst, dass im Fehlerfall klar verst√§ndliche und spezifische Fehlermeldungen zur√ºckgegeben werden. Beispielsweise werden bei Validierungsfehlern nun alle fehlerhaften Felder mit den jeweiligen Fehlermeldungen zur√ºckgegeben.

---

### Probleme und deren L√∂sungen

#### Problem: Ung√ºltige Datenformate
**Herausforderung:** Die API konnte Eingaben mit fehlerhaften oder unvollst√§ndigen Daten nicht angemessen verarbeiten.  
**L√∂sung:** Es wurden umfassende Validierungsregeln implementiert, die sicherstellen, dass nur g√ºltige Eingaben akzeptiert werden.

#### Problem: Zus√§tzliche, nicht erlaubte Felder in Anfragen
**Herausforderung:** Nutzer konnten zus√§tzliche Felder in den Requests senden, die nicht zur API-Spezifikation geh√∂rten.  
**L√∂sung:** Eine explizite Pr√ºfung der √ºbermittelten Felder wurde eingef√ºhrt. Bei Abweichungen wird ein 400-Fehler (Bad Request) zur√ºckgegeben.

#### Problem: Unspezifische Fehlermeldungen bei PUT-/POST-Anfragen
**Herausforderung:** Die API gab teilweise unspezifische Fehlermeldungen wie ‚ÄûBad Request‚Äú aus, was die Fehlerbehebung erschwerte. Au√üerdem passten die Meldungen und Fehlercodes teilweise nicht zu den bereits vorhandenen Testf√§llen, sodass einige Testf√§lle grunds√§tzlich fehlschlugen.
**L√∂sung:** Die Fehlermeldungen wurden so √ºberarbeitet, dass sie den genauen Grund des Fehlers benennen, z. B. ‚ÄûTitel muss mindestens 3 Zeichen lang sein‚Äú. Au√üerdem wurden die erwarteten Fehlermeldungen und -Codes in den Testf√§llen korrigiert.

#### Problem: Authentifizierungsprobleme bei Unit-Tests
**Herausforderung:** Einige Tests erforderten ein g√ºltiges Authentifizierungstoken, obwohl der in der Datei `utils.js` angegebene Keycloak-Server nicht erreichbar war und nach R√ºcksprache mit Hr. Gawron auch nicht verwendet werden sollte.
**L√∂sung:** Die Datei `utils.js` wurde entfernt und die Testf√§lle wurden √ºberarbeitet, sodass keine Authentifizierung mehr erforderlich war.

#### Problem: Bearbeitung und L√∂schen von Todos im Frontend nicht m√∂glich
**Herausforderung:** Parallel zur Implementierung der Unit-Tests ist aufgefallen, dass das Bearbeiten und L√∂schen von Todos im Frontend nicht m√∂glich war. Als Ursache stellte sich heraus, dass in den `onclick`-Events der Buttons mit den Klassen `status`, `edit` und `delete` keine Anf√ºhrungszeichen um die ID des jeweiligen Todos vorhanden waren. Dadurch sind je nach verwendetem Browser verschiedene Fehler aufgetreten und in der Browser-Konsole protokolliert worden.
**L√∂sung:** Die fehlenden Anf√ºhrungszeichen wurden erg√§nzt.

#### Problem: Fehlerhafter Umgang mit IDs im Frontend
**Herausforderung:** Ebenfalls parallel zur Implementierung der Unit-Tests fiel auf, dass das Frontend von numerischen IDs ausgegangen ist. Au√üerdem wurde das ID-Feld im Frontend `id` genannt, obwohl der korrekte durch die MongoDB erstellte Feldname `_id` lautet. Dar√ºber hinaus wurde die ID nach dem Speichern im Frontend nicht zur√ºckgesetzt, was zu Problemen bei der Speicherung weiterer Todos gef√ºhrt hat.
**L√∂sung:** Das Frontend wurde angepasst, sodass das ID-Feld den Namen `_id` hat und die ID nicht mehr in einen Integer konvertiert wird. Zus√§tzlich wird die ID nun nach dem Speichern via `evt.target._id = ""` zur√ºckgesetzt.

#### Problem: Fehlercode 500 bei PUT mit nicht existierender ID
**Herausforderung:** Bei der Implementierung der Unit-Tests wurde festgestellt, dass bei einem PUT-Aufruf mit einer nicht existierenden ID ein 500-Fehler (Internal Server Error) auftritt. Korrekt w√§re in diesem Fall allerdings ein 404-Fehler (Not Found).
**L√∂sung:** Die Datei `db.js` wurde angepasst, sodass im Fall von `result.matchedCount === 0` null zur√ºckgegeben wird. Dadurch wird sichergestellt, dass in diesem Fall durch die bereits vorhandene Logik ein 404-Fehler an das Frontend zur√ºckgegeben wird.

## üöÄ DOKUMENTATION ZUR TESTIMPLEMENTIERUNG MIT CYPRESS üìã

Name: Harun Silajdzija | Matrikelnummer: 30265109

### ‚ú® Einf√ºhrung und Setup

Zun√§chst habe ich einen Feature-Branch vom Main-Branch erstellt, nachdem sichergestellt wurde, dass alle Unit-Tests korrekt laufen und keine funktionalen Probleme bestehen.
Anschlie√üend habe ich ein Codespace eingerichtet und im backend-Verzeichnis die notwendigen Abh√§ngigkeiten mit 'npm install' installiert. Die Anwendung konnte anschlie√üend erfolgreich mit 'npm run start' gestartet werden.

### üß™ Manueller UI-Test und E2E Testabdeckung

Nach einem kurzen manuellen UI-Test, bei dem alle Funktionalit√§ten √ºberpr√ºft wurden, habe ich eine Liste der Testf√§lle erstellt, um sicherzustellen, dass die E2E-Testabdeckung gew√§hrleistet ist:

- Seite todo.html laden
- 3 ToDos hinzuf√ºgen:
    - Fall 1: Datum vor einem Tag + Status ‚Äûerledigt‚Äù
    - Fall 2: Datum heute + Status ‚Äûin Bearbeitung‚Äù
    - Fall 3: Datum 1 Tag sp√§ter + Status ‚Äûoffen‚Äù
    - Erwartetes Ergebnis: Es werden 3 ToDos gelistet.
- [Bearbeiten Button] ToDo 2 bearbeiten (Name, Datum, Status)
    - Erwartetes Ergebnis: Die neuen Daten werden korrekt im ToDo angezeigt.
- [Status Button] Einmal Status √§ndern
    - Erwartetes Ergebnis: Der neue Status wird korrekt f√ºr das ToDo in der Liste angezeigt.
- [L√∂schen Button] L√∂schen aller Testdaten

### üîß Integration von Cypress

Als n√§chstes habe ich das Cypress npm-Package installiert und eine cypress.config.js-Datei erstellt. Diese basierte auf der Konfiguration der vorherigen √úbung. Die Installation erfolgte im Backend-Node-Projekt.

Die End-to-End-Tests befinden sich in der Datei backend/cypress/e2e/todo.cy.js. Zus√§tzlich gibt es in der Datei backend/cypress/support/e2e.js eine Grundkonfiguration, die f√ºr alle Tests geladen wird. Sie sorgt daf√ºr, dass der Zustand des Browsers bereinigt wird, indem alle Cookies und der lokale Speicher gel√∂scht werden. Anschlie√üend wird die Zielseite der Anwendung, in diesem Fall die todo.html, aufgerufen. Diese vorbereitenden Schritte gew√§hrleisten, dass jeder Test in einer sauberen Umgebung startet, um zuverl√§ssige und wiederholbare Ergebnisse zu erm√∂glichen.

Zus√§tzlich habe ich folgende Skripte in die package.json-Datei aufgenommen, um Cypress-Tests bequem ausf√ºhren zu k√∂nnen:
    "scripts": {
        "cy:open": "cypress open",
        "cy:run": "cypress run"
    }
Mit diesen Skripten kann man die Cypress Testlauf-Umgebung entweder im interaktiven Modus (cy:open) oder im Headless-Modus (cy:run) starten.

Beim ersten Testversuch mit Cypress trat eine Fehlermeldung auf, die darauf hinwies, dass Github beim Besuch der App-Seite eine Authentifizierung verlangt. Daher war es notwendig, den Port der Applikation im Codespace √∂ffentlich freizugeben.

### ‚úÖ Test-Erstellung und -Verifikation

Nach der erfolgreichen Integration von Cypress habe ich die Tests gem√§√ü des oben genannten Testplans erstellt.
Beim Anlegen der ToDos habe ich darauf geachtet, die Attribute objekt-spezifisch zu pr√ºfen, da es mehrere ToDos mit dem gleichen Datum und Status geben k√∂nnte.
Da die Tests echte Objekte in der Datenbank erstellen, konnte ich neben den Logmeldungen auch direkt in der Applikation √ºberpr√ºfen, ob die Tests korrekt ausgef√ºhrt wurden.

### üóëÔ∏è Testdaten entfernen

Nachdem alle Tests implementiert und verifiziert waren, habe ich die Testdaten entfernt, um sicherzustellen, dass die Datenbank wieder im urspr√ºnglichen Zustand ist.

## Dokumentation zur Erstellung des CI/CD-Workflows
* Name: Philipp Spiertz
* Matrikelnummer: 30253272

### Einleitung
Um die bereits erstellten Tests f√ºr das Frontend und das Backend so wie die Codeanalyse mit Hilfe von SonarQube automatisiert bei jeder √Ñnderung laufen zu lassen ist es n√∂tig, einen Workflow in GitHub anzulegen. Ein Workflow wird mit Hilfe einer yml-Datei angelegt. Diese muss im Verzeichnis .github/workflows erstellt werden. Es k√∂nnen beliebig viele Workflows angelegt werden, die dann bei bestimmten Events ausgef√ºhrt werden.

Ich habe mich dazu entschieden, alle Workflows in eine Konfigurationsdatei zu packen und dann alle Tests parallel laufen zu lassen und die Tests bei allen Aktionen auszuf√ºhren.

Auf die Konfiguration der einzelnen Jobs jobs wird in den Folgenden Abschnitten eingegangen.

### Tests f√ºr das Backend
Um das Backend zu testen, wurde als Basis f√ºr den Workflow ein Ubuntu-Image verwendet, was in `runs-on` angegeben wird. 

Da das Backend eine MongoDB-Datenbank ben√∂tigt und diese nicht weggemockt wurde, wird diese als Service in `services` hinzugef√ºgt und ist mit den Standardwerten konfiguriert. Es w√§re m√∂glich, einen Nutzernamen und ein Passwort zu setzen, dies ist hier aber nicht n√∂tig.

Damit MongoDB vom Backend erreicht werden kann, ist es n√∂tig, die Ports hierf√ºr anzugeben.

Als n√§chstes wird in `defaults` das Arbeitsverzeichnis angegeben, welches verwendet werden soll. F√ºr diesen Job ist das Verzeichnis `backend`.

Nun werden die einzelnen Schritte in `steps` angegeben, die ben√∂tigt werden, um das Backend zu testen. Zuerst muss der Code ausgecheckt werden, was mit dem importierten Workflow `actions/checkout@v4` passiert.

Nachdem der Code im Workflow vorhanden ist, wird noch mit Hilfe von `actions/setup-node@v4` Node.js als JavaScript-Runtime in Version 20 hinzugef√ºgt.

Damit das Backend gebaut bzw. die Tests ausgef√ºhrt werden k√∂nnen, werden die Abh√§ngigkeiten mit Hilfe von `npm i` installiert.

Danach k√∂nnen die Tests mit `npm run test` ausgef√ºhrt werden. Wenn alles in Ordnung ist, dann quittiert GitHub die erfolgreiche Ausf√ºhrung mit einem gr√ºnen Symbol in der Actions-√úbersicht. Wie das aussieht, wird am Ende noch einmal gezeigt.

Konfiguration f√ºr die Backend-Tests:
```yml
backend-tests:
  runs-on: ubuntu-latest
  services:
    mongo:
      image: mongo
      ports:
        - 27017:27017
  defaults:
    run:
      working-directory: ./backend
  steps:
    - name: Check out repository
      uses: actions/checkout@v4
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: 20
    - name: Install dependencies
      run: npm i
    - name: Run backend tests
      run: npm run test
```
### Tests f√ºr das Frontend
Da das Frontend vom Backend als statische Dateien ausgeliefert wird, ist hierf√ºr keine weitere Konfiguration notwendig. Somit sieht der Workflow f√ºr das Frontend bis auf den Befehl f√ºr die Ausf√ºhrung der Tests identisch aus. Hier wird der Befehl `npm run cy:run` verwendet um Cypress zu starten und die Tests auszuf√ºhren.

Konfiguration f√ºr die Frontend-Tests:
```yml
frontend-tests:
  runs-on: ubuntu-latest
  services:
    mongo:
      image: mongo
      ports:
        - 27017:27017
  defaults:
    run:
      working-directory: ./backend
  steps:
    - name: Check out repository
      uses: actions/checkout@v4
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: 20
    - name: Install dependencies
      run: npm i
    - name: Start application
      run: npm run start &
    - name: Run Cypress tests
      run: npm run cy:run
```
### Codeanalyse mit SonarQube
Als letztes wird noch die Konfiguration von SonarQube vorgenommen. SonarQube ist eine Software, um Softwarequalit√§t festzustellen und bietet auch die M√∂glichkeit, Vorschl√§ge f√ºr die Verbesserung des Quellcodes zu machen. So kann w√§hrend des Entwicklungsprozesses schon schnell f√ºr eine hohe Softwarequalit√§t gesorgt werden.

Um SonarQube verwenden zu k√∂nnen ist es notwendig, ein neues Projekt in SonarQube anzulegen und sich dort dann einen Token f√ºr den Zugriff aus GitHub zu erstellen. Der Token und die Url der SonarQube-Instanz werden dann in die Secrets f√ºr die Actions des GitHub Projektes eingetragen und sind dann in jedem Durchlauf eines Workflows vorhanden und k√∂nnen verwendet werden, wie es in der Konfiguration ersichtlich ist.

Weiter musste noch der Key angegeben werden, der bei der Registrierung in SonarQube angegeben wurde. In diesem Fall `todo-reihe-links`. Als Vorlage f√ºr diesen Workflow diente der offizielle, [von SonarQube zur Verf√ºgung gestellte](https://github.com/marketplace/actions/official-sonarqube-scan) Worflow.

Konfiguration von SonarQube:
```yml
sonar-analysis:
  runs-on: ubuntu-latest
  permissions:
    pull-requests: read
  steps:
    - name: Check out repository
      uses: actions/checkout@v4
    - name: SonarQube analysis
      uses: SonarSource/sonarqube-scan-action@master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
      with:
        args:
          -Dsonar.projectKey=todo-reihe-links
```

### Resultat
Nachdem die automatisierten Tests ausgef√ºhrt wurden, kann nun der Actions-√úbersicht entnommen werden, ob diese erfolgreich waren.

![Actions √úbersicht](image.png)

Auf dem Bild sind erfolgreiche wie auch nicht erfolgreiche Durchl√§ufe der Workflows zu sehen.

Wenn man einen Durchlauf anklickt, k√∂nnen auch die Ergebnisse der einzelnen Jobs eines Worflows angezeigt werden.

![Action Drilldown](image-1.png)
In diesem Bild sind die drei Jobs ersichtlich, die im Workflow f√ºr die einzelnen Aufgaben erfolgreich ausgef√ºhrt wurden. Hier ist es so, dass im normalen Betrieb nur die Tests f√ºr Frontend und Backend fehlschlagen k√∂nnen. Um Informationen √ºber die Ergebnisse von SonarQube zu erhalten, muss daf√ºr das Webinterface von SonarQube aufgerufen werden.

![√úbersicht SonarQube](image-2.png)

Wenn man nach dem Projekt in SonarQube sucht, kann man hier schon eine √úbersicht √ºber ein paar Metriken erhalten.

Wenn man das Projekt ausw√§hlt, kann man tiefer in die Probleme einsteigen, die SonarQube gefunden hat.

Was auff√§llt ist, dass SonarQube speziell f√ºr dieses Projekt viele Fehlalarme ausl√∂st, da Todos in produktiv eingesetztem Code nicht erw√ºnscht sind. Dadurch, dass es sich bei der analysierten Anwendung um eine Todo-App handelt, kommt das Wort Todo aber h√§ufiger vor. Diese Fehlalarme k√∂nnen ignoriert werden.

![alt text](image-3.png)

Weiter findet SonarQube aber auch Probleme im Code selbst.

![const an Stelle von let](image-4.png)

So schl√§gt SonarQube Refactorings vor, die den Code konsistenter machen.

Zus√§tzlich findet SonarQube auch Stellen im Code, in der z.B. Passw√∂rter hart einprogrammiert sind. Zum Zeitpunkt der Erstellung dieser Dokumentation war das nicht mehr der Fall. Im urspr√ºnglichen Code fand SonarQube aber genau solch ein Problem.

Zusammenfassend ist zu sagen, dass durch den Einsatz von SonarQube im Code schon viele Probleme erkannt werden k√∂nnen, ohne dass ein Entwickler nach jedem Commit ein genaues Codereview durchf√ºhren muss. Trotzdem erspart SonarQube nicht, die Software auch manuell zu testen und den Code unter den Gesichtspunkten von z.B. der Fachlichkeit auf Korrektheit zu untersuchen.

### Probleme beim Hinzuf√ºgen der Workflows
Beim Hinzuf√ºgen der Workflows kam es nur zu kleineren Problemen.

Am Anfang zeigte mir SonarQube an, dass bei der Analyse keine Quellcode-Dateien gefunden werden konnten. Das lag daran, dass der Code nicht ausgecheckt wurde, da dies explizit geschehen muss. Danach meldete SonarQube kein solches Problem mehr.

Ein weiteres Problem war die Art, wie das Setup f√ºr die Tests im Frontend durchgef√ºhrt wurde. Das f√ºhrte dazu, dass die Tests immer failten, weil eine Bedingung im Workflow wie auch auch meinem Computer niemals zutraf.

Bis auf diese zwei kleinen Probleme war die Implementierung aber sehr einfach und sorgte f√ºr das Verst√§ndnis, dass es einfach ist Tools einzusetzen, die helfen, Code besser zu machen.